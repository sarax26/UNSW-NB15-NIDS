{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b8b8ea",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION USING MUTUAL INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ee94c",
   "metadata": {},
   "source": [
    "Mutual information (MI) is preferred for feature selection in KNN modeling because it can capture both linear and nonlinear relationships between features and the target. It's robust, doesn't assume a specific data distribution, and selects features based on how much information they provide for classification. MI is a versatile choice for KNN and is suitable for datasets with complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f19c9",
   "metadata": {},
   "source": [
    "Loading the Dataset: The code begins by importing the necessary libraries and loading the dataset from a CSV file using Pandas.\n",
    "\n",
    "Defining Features and Target: It defines the features (X) and the target variable (y) for your machine learning task. In this case, it excludes the 'id' and 'label' columns from the dataset as they are not considered as features.\n",
    "\n",
    "Identifying Numerical and Categorical Columns:\n",
    "\n",
    "Numerical columns are those that contain continuous or discrete numeric values.\n",
    "Categorical columns are those that contain categorical or text-based values.\n",
    "Preprocessing for Numerical Features:\n",
    "\n",
    "It sets up a preprocessing pipeline for numerical features.\n",
    "The pipeline includes:\n",
    "SimpleImputer: Imputes missing values with the median of each numerical column.\n",
    "StandardScaler: Standardizes (scales) the numerical features to have a mean of 0 and a standard deviation of 1.\n",
    "Preprocessing for Categorical Features:\n",
    "\n",
    "It sets up a preprocessing pipeline for categorical features.\n",
    "The pipeline includes:\n",
    "SimpleImputer: Imputes missing values with the most frequent value in each categorical column.\n",
    "OneHotEncoder: Performs one-hot encoding to convert categorical values into binary (0 or 1) format.\n",
    "Combining Preprocessing Steps:\n",
    "\n",
    "It uses the ColumnTransformer to combine the preprocessing steps for both numerical and categorical columns.\n",
    "This ensures that all features are processed correctly before feature selection.\n",
    "Feature Selection:\n",
    "\n",
    "It specifies the number of features to select (num_features_to_select) based on your requirements.\n",
    "It checks if the specified number of features is not greater than the total number of available features.\n",
    "It uses SelectKBest with the f_classif score function to select the top features.\n",
    "The fit_transform method is called on the combined preprocessing and feature selection steps to perform feature selection on the dataset.\n",
    "It then retrieves a mask of selected features using selector.get_support().\n",
    "Getting Selected Feature Names:\n",
    "\n",
    "It constructs a list of all feature names, including numerical and one-hot encoded categorical features.\n",
    "It filters this list to include only the names of the selected features based on the mask obtained in the previous step.\n",
    "Displaying Selected Features:\n",
    "\n",
    "Finally, it prints the names of the selected features to the console.\n",
    "This code effectively prepares your dataset by preprocessing both numerical and categorical features, selects the top K features based on mutual information, and displays the names of those selected features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da60a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "rate\n",
      "sttl\n",
      "dload\n",
      "swin\n",
      "dmean\n",
      "ct_state_ttl\n",
      "ct_dst_sport_ltm\n",
      "proto_tcp\n",
      "state_CON\n",
      "state_INT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaik\\OneDrive\\Documents\\Python Scripts\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv('C:/Users/shaik/Downloads/USNW (1).csv')\n",
    "data\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(['id', 'label'], axis=1)  # Exclude 'id' and 'label' columns\n",
    "y = data['label']\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_cols = ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload',\n",
    "                  'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
    "                  'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len',\n",
    "                  'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
    "                  'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n",
    "categorical_cols = ['proto', 'service', 'state']\n",
    "\n",
    "# Apply preprocessing to numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Replace missing values with median\n",
    "    ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "])\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Replace missing values with the most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for numerical and categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Combine preprocessing with feature selection\n",
    "num_features_to_select = 10  # Adjust this based on your needs, but ensure it's less than the total number of available features\n",
    "if num_features_to_select > X.shape[1]:\n",
    "    num_features_to_select = X.shape[1]\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
    "\n",
    "# Fit the preprocessing and feature selection steps\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "selector.fit(X_preprocessed, y)\n",
    "\n",
    "# Get the mask of selected features\n",
    "selected_feature_indices = selector.get_support()\n",
    "\n",
    "# Get the names of the selected features\n",
    "all_feature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_cols))\n",
    "selected_features = [all_feature_names[i] for i, is_selected in enumerate(selected_feature_indices) if is_selected]\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977f445",
   "metadata": {},
   "source": [
    "# K-NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0091b",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries:\n",
    "\n",
    "The code begins by importing the required libraries, including train_test_split for splitting the dataset, KNeighborsClassifier for creating the KNN model, and accuracy_score for evaluating model accuracy.\n",
    "Splitting the Data:\n",
    "\n",
    "It uses train_test_split to split the preprocessed dataset into training and testing sets.\n",
    "The test_size parameter specifies the proportion of data to be used for testing (in this case, 20%).\n",
    "The random_state parameter is set for reproducibility.\n",
    "Creating the KNN Model:\n",
    "\n",
    "It initializes a KNN classifier by specifying the number of neighbors (k) to consider when making predictions.\n",
    "You can adjust the value of k based on your needs. A larger k considers more neighbors, while a smaller k focuses on fewer neighbors.\n",
    "Fitting the KNN Model:\n",
    "\n",
    "The KNN model is trained (fitted) using the training data (X_train and y_train) to learn the underlying patterns in the data.\n",
    "Making Predictions:\n",
    "\n",
    "After training, the model is used to make predictions on the test set (X_test).\n",
    "The predictions are stored in the y_pred variable.\n",
    "Evaluating Model Accuracy:\n",
    "\n",
    "It calculates the accuracy of the KNN model using the accuracy_score function.\n",
    "The accuracy score is a measure of how well the model's predictions match the actual target values.\n",
    "The result is printed to the console to show the accuracy of the KNN model on the test data.\n",
    "In summary, this code demonstrates how to split a preprocessed dataset into training and testing sets, create and train a KNN classifier, use it to make predictions, and evaluate the model's accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca52ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2020afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed[:, selected_feature_indices], y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8ca1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit the KNN model\n",
    "k = 10  # You can adjust the number of neighbors (k) as needed\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ffc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c647c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN model: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of KNN model: {accuracy:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599c283",
   "metadata": {},
   "source": [
    "Accuracy: Accuracy is a common evaluation metric for classification models. It measures the proportion of correctly classified instances (or data points) out of the total number of instances in the test dataset.\n",
    "\n",
    "0.93: The value \"0.93\" represents the accuracy score, which is a decimal number between 0 and 1. In this case, it means that the KNN model correctly classified approximately 93% of the instances in the test dataset.\n",
    "\n",
    "The accuracy score is calculated as:\n",
    "\n",
    "Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n",
    "\n",
    "In this case, the KNN model made correct predictions for about 93% of the instances in the test dataset.\n",
    "\n",
    "A higher accuracy score indicates that the model is performing well in terms of classifying the data correctly. In this context, an accuracy of 0.93 is generally considered quite good, as it means the model is accurate in its predictions for a large majority of the test instances.\n",
    "\n",
    "However, it's essential to consider the specific problem and dataset when interpreting accuracy. In some cases, other metrics such as precision, recall, or F1-score may also be important, especially when dealing with imbalanced datasets or specific requirements of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0b46e",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION FOR RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd80949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "sttl\n",
      "ct: state_ttl\n",
      "dload\n",
      "rate\n",
      "sload\n",
      "dttl\n",
      "dmean\n",
      "ackdat\n",
      "smean\n",
      "ct: srv_dst\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv('C:/Users/shaik/Downloads/USNW (1).csv')\n",
    "\n",
    "# Define the target variable ('label')\n",
    "target = data['label']\n",
    "\n",
    "# Define the features (exclude 'id', 'attack_cat', and 'label' columns)\n",
    "X = data.drop(['id', 'attack_cat', 'label'], axis=1)\n",
    "\n",
    "# Define categorical columns (if any)\n",
    "categorical_cols = ['proto', 'service', 'state']\n",
    "\n",
    "# Label encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "random_forest_model.fit(X, target)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = random_forest_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top features based on importance in the desired format\n",
    "top_n = 10  # Change this to the desired number of top features\n",
    "selected_features = feature_importance_df['Feature'][:top_n].tolist()\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    if '_' in feature:\n",
    "        category_value = feature.split('_')\n",
    "        category = category_value[0]\n",
    "        value = '_'.join(category_value[1:])\n",
    "        print(f\"{category}: {value}\")\n",
    "    else:\n",
    "        print(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb83bdf",
   "metadata": {},
   "source": [
    "# RANDOM FOREST MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f51ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Performance:\n",
      "Accuracy: 0.9523225640879409\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     11169\n",
      "           1       0.96      0.98      0.97     23900\n",
      "\n",
      "    accuracy                           0.95     35069\n",
      "   macro avg       0.95      0.94      0.94     35069\n",
      "weighted avg       0.95      0.95      0.95     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv('C:/Users/shaik/Downloads/USNW (1).csv')\n",
    "\n",
    "# Define the target variable ('label')\n",
    "target = data['label']\n",
    "\n",
    "# Define the selected features (based on importance)\n",
    "selected_features = [\n",
    "    'sttl',\n",
    "    'ct_state_ttl',\n",
    "    'dload',\n",
    "    'rate',\n",
    "    'sload',\n",
    "    'dttl',\n",
    "    'dmean',\n",
    "    'ackdat',\n",
    "    'smean',\n",
    "    'ct_srv_dst'\n",
    "]\n",
    "\n",
    "# Extract the selected features from the dataset\n",
    "X = data[selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff568225",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION FOR DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd4f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['state', 'rate', 'sttl', 'dload', 'swin', 'dwin', 'dmean', 'ct_state_ttl', 'ct_src_dport_ltm', 'ct_dst_sport_ltm']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv('C:/Users/shaik/Downloads/USNW (1).csv')\n",
    "\n",
    "# Define the target variable ('label')\n",
    "target = data['label']\n",
    "\n",
    "# Define the features (exclude 'id', 'attack_cat', and 'label' columns)\n",
    "X = data.drop(['id', 'attack_cat', 'label'], axis=1)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Apply label encoding to categorical columns (or you can use one-hot encoding)\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Apply feature selection (SelectKBest with f_classif score)\n",
    "num_features_to_select = 10  # Adjust the number of features to select\n",
    "selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
    "X_selected = selector.fit_transform(X, target)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_features = X.columns[selected_feature_indices].tolist()\n",
    "\n",
    "# Display the selected feature names\n",
    "print(\"Selected Features:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56d24f",
   "metadata": {},
   "source": [
    "# DECISION TREE MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50b40090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Performance:\n",
      "Accuracy: 0.9258889617610996\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88     11169\n",
      "           1       0.94      0.95      0.95     23900\n",
      "\n",
      "    accuracy                           0.93     35069\n",
      "   macro avg       0.92      0.91      0.91     35069\n",
      "weighted avg       0.93      0.93      0.93     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Decision Tree classifier\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Decision Tree Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe92113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
