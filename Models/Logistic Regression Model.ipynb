{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e1ae51",
   "metadata": {},
   "source": [
    "### Feature selection- top 10 IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb44b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 variables with highest IV values:\n",
      "IV for sload: 0.9558\n",
      "IV for dur: 0.9166\n",
      "IV for rate: 0.9141\n",
      "IV for dload: 0.8384\n",
      "IV for dinpkt: 0.8230\n",
      "IV for sbytes: 0.7933\n",
      "IV for sinpkt: 0.7580\n",
      "IV for sjit: 0.6902\n",
      "IV for sttl: 0.6788\n",
      "IV for dbytes: 0.6721\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "\n",
    "data = pd.read_csv('UNSW.csv')\n",
    "\n",
    "X = data.drop(['id', 'label', 'attack_cat'], axis=1)  # Exclude 'id', 'label', and 'attack_cat' columns\n",
    "y = data['label']\n",
    "\n",
    "#one hot encoding the categorical variables\n",
    "categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "iv_threshold = 0.3 \n",
    "iv_values = []\n",
    "\n",
    "for col in X_train.columns:\n",
    "    crosstab = pd.crosstab(X_train[col], y_train)\n",
    "    chi2, _, _, _ = stats.chi2_contingency(crosstab)\n",
    "    iv = chi2 / X_train.shape[0]\n",
    "    if iv > iv_threshold:\n",
    "        iv_values.append((col, iv))\n",
    "\n",
    "sorted_iv_values = sorted(iv_values, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 10 variables with the highest IV values\n",
    "top_10_variables = [feature for feature, _ in sorted_iv_values[:10]]\n",
    "\n",
    "print(\"Top 10 variables with highest IV values:\")\n",
    "for feature, iv in sorted_iv_values[:10]:\n",
    "    print(f\"IV for {feature}: {iv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de728651",
   "metadata": {},
   "source": [
    "### model of Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c957e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.837948045282158\n",
      "Overall Precision: 0.8357667675106653\n",
      "Overall Recall: 0.837948045282158\n",
      "Overall F1-score: 0.8364762716587316\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "\n",
    "data = pd.read_csv('UNSW.csv')\n",
    "\n",
    "top_features = ['sload', 'dur', 'rate', 'dload', 'dinpkt', 'sbytes', 'sinpkt', 'sjit', 'sttl', 'dbytes']\n",
    "\n",
    "X = data[top_features]\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Overall Precision:\", precision)\n",
    "print(\"Overall Recall:\", recall)\n",
    "print(\"Overall F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58a394",
   "metadata": {},
   "source": [
    "#### After Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae1c76d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.885112207362628\n",
      "Overall Precision: 0.886804001985894\n",
      "Overall Recall: 0.885112207362628\n",
      "Overall F1-Score: 0.8810820446636886\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('UNSW.csv')\n",
    "\n",
    "#  top 10 features with the highest IV values\n",
    "top_features = ['sload', 'dur', 'rate', 'dload', 'dinpkt', 'sbytes', 'sinpkt', 'sjit', 'sttl', 'dbytes']\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into train:test 80:20 \n",
    "X_train, X_test, y_train, y_test = train_test_split(data[top_features], y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_logistic_regression_model = None\n",
    "\n",
    "# Define hyperparameters to search\n",
    "hyperparameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "for C in hyperparameters['C']:\n",
    "    for penalty in hyperparameters['penalty']:\n",
    "        for solver in hyperparameters['solver']:\n",
    "            # Initialize the logistic regression model\n",
    "            logistic_regression_model = LogisticRegression(C=C, penalty=penalty, solver=solver)\n",
    "            \n",
    "            # Fit the logistic regression model on the training data\n",
    "            logistic_regression_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict labels on the test set using the logistic regression model\n",
    "            y_pred = logistic_regression_model.predict(X_test)\n",
    "            \n",
    "            # Evaluate the model's accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_logistic_regression_model = logistic_regression_model\n",
    "\n",
    "\n",
    "best_y_pred = best_logistic_regression_model.predict(X_test)\n",
    "classification_rep = classification_report(y_test, best_y_pred, output_dict=True)\n",
    "\n",
    "\n",
    "overall_precision = classification_rep['weighted avg']['precision']\n",
    "overall_recall = classification_rep['weighted avg']['recall']\n",
    "overall_f1_score = classification_rep['weighted avg']['f1-score']\n",
    "\n",
    "# Print the best accuracy, overall precision, recall, and F1-score\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Overall Precision:\", overall_precision)\n",
    "print(\"Overall Recall:\", overall_recall)\n",
    "print(\"Overall F1-Score:\", overall_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
